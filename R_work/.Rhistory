corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
inspect(sms_corpus[1:20])
inspect(corpus_clean)
inspect(corpus_clean[1:20])
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_dtm
inspect(sms_dtm)
spam_data_train <- spam_data[1:4169, ]
spam_data_test <- spam_data[4170:5559, ]
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_corpus_train <- corpus_clean[1:4169, ]
sms_corpus_test <- corpus_clean[4170:5559, ]
sms_corpus_train <- corpus_clean[1:4169, ]
sms_corpus_train <- corpus_clean[1:4169]
sms_corpus_test <- corpus_clean[4170:5559]
prop.table(table(spam_data_train$type))
prop.table(table(spam_data_test$type))
library(wordcloud)
wordcloud(sms_corpus_train, min.freq = 30, random.order = F)
spam <- subset(spam_data_train, type="ham")
spam <- subset(spam_data_train, type="spam")
ham <- subset(spam_data_train, type="ham")
wordcloud(spam$text, min.freq = 40, scale = c(3, 0.5))
wordcloud(ham$text, min.freq = 40, scale = c(3, 0.5))
findFreqTerms(sms_dtm_train, 5) # 빈도수가 다섯개 이상인 용어 찾기
ind <- which(spam_data$type == "spam")
train_spam = corpus_clean[(ind <= 4169), ]
findFreqTerms(sms_dtm_train, 5) # 빈도수가 다섯개 이상인 용어 찾기
ind <- which(spam_data$type == "spam")
train_spam = corpus_clean[(ind <= 4169), ]
train_spam = corpus_clean[(ind <= 4169)]
conver_count <- function(x) {
x <- ifelse(x>0, 1, 0)
x <- factor(x, levels = c(0,1),
labels = c("No", "Yes"))
}
sms_train <- apply(sms_dtm_train,
MARGIN = 2,
conver_count)
sms_test <- apply(sms_dtm_test,
MARGIN = 2,
conver_count)
View(sms_test)
inspect(sms_dtm[1:20])
inspect(sms_dtm)
sms_dtm_train_has <- apply(sms_dtm_train,
MARGIN = 2,
conver_count)
sms_dtm_test_has <- apply(sms_dtm_test,
MARGIN = 2,
conver_count)
spam_data = read.csv("sms_spam.csv", header = T, fileEncoding="UTF-8")
spam_data$type <- factor(spam_data$type)
spam_data$text <- str_to_lower(spam_data$text)
sms_corpus <- Corpus(VectorSource(spam_data$text))
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
sms_dtm <- DocumentTermMatrix(corpus_clean)
spam_data_train <- spam_data[1:4169, ]
spam_data_test <- spam_data[4170:5559, ]
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_corpus_train <- corpus_clean[1:4169]
sms_corpus_test <- corpus_clean[4170:5559]
inspect(corpus_clean[1:20])
str(corpus_clean)
inspect(corpus_clean[1:20])
library(e1071)
start_time = Sys.time()
sms_classifier <- naiveBayes(sms_dtm_train, spam_data_train$type)
delay = Sys.time() - start_time
start_time = Sys.time()
sms_classifier <- naiveBayes(sms_dtm_train, spam_data_train$type)
sms_classifier <- naiveBayes(sms_dtm_train_has, spam_data_train$type)
sms_dtm_train_has <- apply(sms_dtm_train,
MARGIN = 2,
conver_count)
conver_count <- function(x) {
x <- ifelse(x>0, 1, 0)
x <- factor(x, levels = c(0,1),
labels = c("No", "Yes"))
}
sms_dtm_train_has <- apply(sms_dtm_train,
MARGIN = 2,
conver_count)
sms_dtm_test_has <- apply(sms_dtm_test,
MARGIN = 2,
conver_count)
start_time = Sys.time()
sms_classifier <- naiveBayes(sms_dtm_train_has, spam_data_train$type)
delay = Sys.time() - start_time
delay
sms_classifier$apriori
sms_test_pred <- predict(sms_classifier, sms_dtm_test_has)
sms_test_pred <- predict(sms_classifier, spam_data_test$type)
sms_test_pred
sms_test_pred <- predict(sms_classifier, spam_data_test)
naiveBayes(Species~., data=iris)
iris_classifier = naiveBayes(Species~., data=iris)
pred_iris = predict(iris_classifier, iris)
table(iris$Species, pred_iris)
ind = sample(1:nrow(iris),
nrow(iris)*0.7,
replace = F)
train = iris[ind, ]
test = iris[-ind, ]
table(test$Species, pred_iris)
pred_iris = predict(iris_classifier, test)
iris_classifier = naiveBayes(Species~., data=train)
pred_iris = predict(iris_classifier, test)
table(test$Species, pred_iris)
##### 4장 : 나이브 베이즈(Naive Bayes)를 사용한 분류 --------------------
## 예제 : 스팸 SMS 메시지 제거 ----
## 2 단계 : 데이터 준비와 살펴보기  ----
# sms 데이터 프레임으로 sms 데이터 읽기
sms_raw <- read.csv("sms_spam.csv", stringsAsFactors = FALSE)
# sms 데이터 구조
str(sms_raw)
# 팩터로 spam/ham으로 변환
sms_raw$type <- factor(sms_raw$type)
# 변수형 확인
str(sms_raw$type)
table(sms_raw$type)
# 텍스트 마이닝(tm) 패키지를 사용하여 말뭉치 생성
library(tm)
sms_corpus <- Corpus(VectorSource(sms_raw$text))
# sms 말뭉치 확인
print(sms_corpus)
inspect(sms_corpus[1:3])
# tm_map() 사용하여 말뭉치 정리
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
# 말뭉치 정리 확인
inspect(sms_corpus[1:3])
inspect(corpus_clean[1:3])
# 문서-용어 희소 매트릭스 생성
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_dtm
# 훈련과 테스트 데이터셋 생성
sms_raw_train <- sms_raw[1:4169, ]
sms_raw_test  <- sms_raw[4170:5559, ]
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test  <- sms_dtm[4170:5559, ]
sms_corpus_train <- corpus_clean[1:4169]
sms_corpus_test  <- corpus_clean[4170:5559]
# 스팸 비율 확인
prop.table(table(sms_raw_train$type))
prop.table(table(sms_raw_test$type))
# 단어 클라우드 시각화
library(wordcloud)
wordcloud(sms_corpus_train, min.freq = 30, random.order = FALSE)
# 훈련 데이터를 스팸과 햄으로 구분
spam <- subset(sms_raw_train, type == "spam")
ham  <- subset(sms_raw_train, type == "ham")
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
# 빈번한 단어에 대한 속성 지시자
findFreqTerms(sms_dtm_train, 5)
sms_dict <- Dictionary(findFreqTerms(sms_dtm_train, 5))
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary = sms_dict))
sms_test  <- DocumentTermMatrix(sms_corpus_test, list(dictionary = sms_dict))
# 개수를 팩터로 변환
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)
x <- factor(x, levels = c(0, 1), labels = c("No", "Yes"))
}
# apply() convert_counts()를 사용한 훈련/테스트 데이터 추출
sms_train <- apply(sms_train, MARGIN = 2, convert_counts)
sms_test  <- apply(sms_test, MARGIN = 2, convert_counts)
## 3 단계 : 데이터로 모델 훈련 ----
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
sms_classifier
## 4 단계 : 모델 성능 평가 ----
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels)
CrossTable(sms_test_pred, sms_raw_test$type,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
## 5 단계 : 모델 성능 향상 ----
sms_classifier2 <- naiveBayes(sms_train, sms_raw_train$type, laplace = 1)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
CrossTable(sms_test_pred2, sms_raw_test$type,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
##### 4장 : 나이브 베이즈(Naive Bayes)를 사용한 분류 --------------------
## 예제 : 스팸 SMS 메시지 제거 ----
## 2 단계 : 데이터 준비와 살펴보기  ----
# sms 데이터 프레임으로 sms 데이터 읽기
sms_raw <- read.csv("sms_spam.csv", stringsAsFactors = FALSE)
# sms 데이터 구조
str(sms_raw)
# 팩터로 spam/ham으로 변환
sms_raw$type <- factor(sms_raw$type)
# 변수형 확인
str(sms_raw$type)
table(sms_raw$type)
# 텍스트 마이닝(tm) 패키지를 사용하여 말뭉치 생성
library(tm)
sms_corpus <- Corpus(VectorSource(sms_raw$text))
# sms 말뭉치 확인
print(sms_corpus)
inspect(sms_corpus[1:3])
# tm_map() 사용하여 말뭉치 정리
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
# 말뭉치 정리 확인
inspect(sms_corpus[1:3])
inspect(corpus_clean[1:3])
# 문서-용어 희소 매트릭스 생성
sms_dtm <- DocumentTermMatrix(corpus_clean)
sms_dtm
# 훈련과 테스트 데이터셋 생성
sms_raw_train <- sms_raw[1:4169, ]
sms_raw_test  <- sms_raw[4170:5559, ]
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test  <- sms_dtm[4170:5559, ]
sms_corpus_train <- corpus_clean[1:4169]
sms_corpus_test  <- corpus_clean[4170:5559]
# 스팸 비율 확인
prop.table(table(sms_raw_train$type))
prop.table(table(sms_raw_test$type))
# 단어 클라우드 시각화
library(wordcloud)
wordcloud(sms_corpus_train, min.freq = 30, random.order = FALSE)
# 훈련 데이터를 스팸과 햄으로 구분
spam <- subset(sms_raw_train, type == "spam")
ham  <- subset(sms_raw_train, type == "ham")
wordcloud(spam$text, max.words = 40, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 40, scale = c(3, 0.5))
# 빈번한 단어에 대한 속성 지시자
findFreqTerms(sms_dtm_train, 5)
sms_dict <- Dictionary(findFreqTerms(sms_dtm_train, 5))
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary = sms_dict))
sms_test  <- DocumentTermMatrix(sms_corpus_test, list(dictionary = sms_dict))
# 개수를 팩터로 변환
convert_counts <- function(x) {
x <- ifelse(x > 0, 1, 0)
x <- factor(x, levels = c(0, 1), labels = c("No", "Yes"))
}
# apply() convert_counts()를 사용한 훈련/테스트 데이터 추출
sms_train <- apply(sms_train, MARGIN = 2, convert_counts)
sms_test  <- apply(sms_test, MARGIN = 2, convert_counts)
## 3 단계 : 데이터로 모델 훈련 ----
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
sms_classifier
## 4 단계 : 모델 성능 평가 ----
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels)
CrossTable(sms_test_pred, sms_raw_test$type,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
## 5 단계 : 모델 성능 향상 ----
sms_classifier2 <- naiveBayes(sms_train, sms_raw_train$type, laplace = 1)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
CrossTable(sms_test_pred2, sms_raw_test$type,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
setwd("/Users/yoon/Documents/DataScience/R_work")
dyn.load("/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server/libjvm.dylib")
library(rJava)
Sys.setlocale("LC_ALL", "ko_KR.UTF-8") # 한글 인코딩 가능하게 해줌
sms_raw_data = read.csv("sms_spam.csv", header = T, fileEncoding="UTF-8")
str(sms_raw_data)
sms_raw_data$type <- factor(sms_raw_data$type)
library(stringr)
sms_raw_data$text <- str_to_lower(sms_raw_data$text)
table(sms_raw_data$type)
sms_corpus <- Corpus(VectorSource(sms_raw_data$text))
inspect(sms_corpus[1:20])
corpus_clean <- tm_map(sms_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
inspect(corpus_clean[1:20])
sms_dtm <- DocumentTermMatrix(corpus_clean)
inspect(sms_dtm)
sms_data_train <- sms_raw_data[1:4169, ]
sms_data_test <- sms_raw_data[4170:5559, ]
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_corpus_train <- corpus_clean[1:4169]
sms_corpus_test <- corpus_clean[4170:5559]
prop.table(table(spam_data_train$type))
prop.table(table(spam_data_test$type))
prop.table(table(sms_data_train$type))
prop.table(table(sms_data_test$type))
spam <- subset(spam_data_train, type="spam")
ham <- subset(spam_data_train, type="ham")
spam <- subset(sms_data_train, type="spam")
ham <- subset(sms_data_test, type="ham")
findFreqTerms(sms_dtm_train, 5) # 빈도수가 다섯개 이상인 용어 찾기
print(sms_corpus)
sms_dict<- findFreqTerms(sms_dtm_train, 5)
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary = sms_dict))
sms_test <- DocumentTermMatrix(sms_corpus_test, list(dictionary = sms_dict))
sms_dict
x <- ifelse(x>0, 1, 0)
conver_count <- function(x) {
x <- ifelse(x>0, 1, 0)
x <- factor(x, levels = c(0,1),
labels = c("No", "Yes"))
}
sms_train <- apply(sms_dtm_train,
MARGIN = 2,
conver_count)
sms_test <- apply(sms_dtm_test,
MARGIN = 2,
conver_count)
sms_train <- apply(sms_dtm_train,
MARGIN = 2,
conver_count)
sms_test <- apply(sms_dtm_test,
MARGIN = 2,
conver_count)
library(e1071)
start_time = Sys.time()
sms_classifier <- naiveBayes(sms_train, sms_raw_train$type)
delay = Sys.time() - start_time
sms_classifier <- naiveBayes(sms_train, sms_data_train$type)
sms_classifier$apriori
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels)
install.packages("gmodels")
library(gmodels)
CrossTable(sms_test_pred, sms_raw_test$type,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
CrossTable(sms_test_pred, sms_data_test$type,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
sms_test_pred <- predict(sms_classifier, sms_test)
convert_count <- function(x) {
x <- ifelse(x>0, 1, 0)
x <- factor(x, levels = c(0,1),
labels = c("No", "Yes"))
}
sms_train <- apply(sms_dtm_train,
MARGIN = 2,
convert_count)
sms_test <- apply(sms_dtm_test,
MARGIN = 2,
convert_count)
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary = sms_dict))
sms_test <- DocumentTermMatrix(sms_corpus_test, list(dictionary = sms_dict))
sms_classifier <- naiveBayes(sms_train, sms_data_train$type)
sms_train <- apply(sms_dtm_train,
MARGIN = 2,
convert_count)
sms_test <- apply(sms_dtm_test,
MARGIN = 2,
convert_count)
sms_classifier <- naiveBayes(sms_train, sms_data_train$type)
sms_classifier
sms_classifier$apriori
table(sms_raw_data$type)
sms_test_pred <- predict(sms_classifier, sms_test)
sms_data_train$type
sms_train
sms_train <- apply(sms_train,
MARGIN = 2,
convert_count)
sms_test <- apply(sms_test,
MARGIN = 2,
convert_count)
sms_dict <- findFreqTerms(sms_dtm_train, 5)
sms_train <- DocumentTermMatrix(sms_corpus_train, list(dictionary = sms_dict))
sms_test <- DocumentTermMatrix(sms_corpus_test, list(dictionary = sms_dict))
convert_count <- function(x) {
x <- ifelse(x>0, 1, 0)
x <- factor(x, levels = c(0,1),
labels = c("No", "Yes"))
}
sms_train <- apply(sms_train,
MARGIN = 2,
convert_count)
sms_test <- apply(sms_test,
MARGIN = 2,
convert_count)
sms_classifier <- naiveBayes(sms_train, sms_data_train$type)
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels)
CrossTable(sms_test_pred, sms_data_test$type,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
install.packages("randomForest")
library(randomForest)
rand1 = randomForest(sms_dtm_train, sms_data_train$type)
as.integer(sms_train[1,1])
sms_train[1,1]
as.factor(sms_train[1,1])
as.integer(as.factor(sms_train[1,1]))
sms_train
View(sms_train)
sms_train2 <- apply(sms_train,
MARGIN = 2,
convert_count)
sms_test2 <- apply(sms_test,
MARGIN = 2,
convert_count)
setwd("/Users/yoon/Documents/DataScience/R_work")
dyn.load("/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server/libjvm.dylib")
library(rJava)
Sys.setlocale("LC_ALL", "ko_KR.UTF-8") # 한글 인코딩 가능하게 해줌
credit = read.csv("credit.csv", header = T, fileEncoding="UTF-8")
credit = read.csv("credit.csv", header = T, fileEncoding="UTF-8")
View(credit)
View(credit)
str(credit)
runif(1000)
order(runif(1000))
(runif(1000)
runif(1000)
runif(1000)
order(runif(1000))
order(c(1,3,2,5))
train = credit[ind[1:900], ]
ind = order(runif(1000))
train = credit[ind[1:900], ]
test = credit[ind[901:1000], ]
install.packages("C50")
library(C50)
model1 = C5.0(default~., train)
plot(model1)
model2 = C5.0(x=train[, c(1,2,5,6)], y=train$default)
model1
model2
plot(model1)
model1 = C5.0(x=train[, -17], y=train[,17])
plot(model1)
model1 = C5.0(x=train[, -17], y=train[,17])
model2 = C5.0(x=train[, c(1,2,5,6)], y=train$default)
summary(model1)
summary(model2)
pred1 = predict(model1, test)
table(test$default, pred1)
pred2 = predict(model2, test)
table(test$default, pred2)
prop.table(test$default, pred1)
library(e1071)
model3 = naiveBayes(defalut~., train)
model3 = naiveBayes(default~., train)
pred3 = predict(model3, test)
table(test$default, pred3)
library(randomForest)
model4 = randomForest(default~., train)
pred4 = predict(model4, test)
table(test$default, pred4)
library(mlbench)
data("Vowel")
nn = nrow(Vowel)
ind = sample(nn, nn*0.7, replace = F)
train = Vowel[ind, ]
test = Vowel[-ind, ]
model1 = C5.0(x=train[, -11], y=train[, 11])
model2 = naiveBayes(x=train[, -11], y=train[, 11])
model3 = randomForest(x=train[, -11], y=train[, 11])
pred1 = predict(model1, test)
pred2 = predict(model2, test)
pred3 = predict(model3, test)
t1 = table(test$Class, pred1)
t1 = table(test$Class, pred2)
t1 = table(test$Class, pred3)
t1 = table(test$Class, pred1)
t2 = table(test$Class, pred2)
t3 = table(test$Class, pred3)
correct1 = sum(diag(t1))/sum(t1)
correct2 = sum(diag(t2))/sum(t2)
correct3 = sum(diag(t3))/sum(t3)
for (i in c(correct1, correct2, correct3)){
out1 = rbind(out1, i)
}
out1 = data.frame()
for (i in c(correct1, correct2, correct3)){
out1 = rbind(out1, i)
}
View(out1)
which.max(out1)
which.max(out1)
View(out1)
colnames(out1) <- c("correct")
out1 = data.frame()
colnames(out1) <- c("correct")
for (i in c(correct1, correct2, correct3)){
out1 = rbind(out1, i)
}
which.max(out1)
View(out1)
colnames(out1) <- c("correct")
View(out1)
which.max(out1$correct)
which.max(out1$correct)
View(out1)
out1[idx,]
out1[idx]
idx = which.max(out1$correct)
out1[idx,]
