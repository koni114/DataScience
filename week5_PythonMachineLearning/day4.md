# 4일차 


-----------------------


#### **1. 차원 축소 기법**

위에서는 피처 선택을 통해 데이터의 차원을 축소했다면, 
이번에는 피처 추출을 통해 원래보다 낮은 차원의 새로운 부분공간으로 변환시켜 데이터를 요약하는
방법을 알아보겠다.

> **1.1 주성분 분석을 활용한 비지도적 차원 축소 기법**

```
여러 변수들의 변량을 주성분 분석(Principal Component)라고 불리는,
서로 상관성이 높은 여러 변수들의 선형조합으로 만든 새로운 변수들로 요약, 축약하는 기법.

회귀분석이나 의사결정트리등의 모델을 만들 때, 다중 공선성의 문제가 발생하는 것을 
쉽게 볼 수 있다. 이런 경우를 해결하는 것이 바로 상관도가 높은 변수들을 하나의 주성분 혹은
요인으로 축소하여 모형개발에 이용하는 것이다.

주성분 분석은 고차원 데이터에서 최대 분산의 방향을 찾아, 
새로운 부분 공간에 원래보다 작은 차원으로 투영하는 것인데, 
이 때PCA는 데이터 각각에 대한 성분을 분석하는 것이 아니라, 
여러 데이터들이 모여 하나의 분포를 이룰 때 이 분포의 주 성분을 분석해 주는 방법이다.

여기서 주성분이라 함은 그 방향으로 데이터들의 분산이 가장 큰 방향벡터를 의미한다.

d차원의 데이터간의 연관성을 찾기 위해 데이터를 먼저 표준화를 시킨다.
피처 상호간의 각각의 공분산을 구하기 위해 공분산 행렬을 만든다.
그리고 공분산행렬을 아이겐벨류와 아이겐벡터로 분해한다.
공분산행렬을 통해 그 두가지를 유도하는 것이 가능한데,

변수들중에 어느 변수가 가장 중요한지를 따지는 것이 아이겐벨류이다.
그리고 아이겐벨류에 따라 나오는 가능한 경우의 수에서 각각마다 나오는 것이 아이겐 벡터이다.
```

>> 공분산이란
```
일반적인 분산은 모집단에서부터 추출한 표본 데이터들의 편차의 제곱의 산술적 평균을 의미하는 것,
즉 평균으로부터 퍼진 정도를 의미한다.

반면 확률론과 통계학에서, 공분산은 2개의 확률변수의 상관정도를 나타내는 값이다.
x와 y의 공분산은 x, y의 흩어진 정도가 얼마나 서로 상관관계를 가지고 흩어졌는지를 나타낸다.

만약 2개의 변수중 하나의 값이 상승하는 경향을 보일 때, 다른 값도 상승하는 경향의 상관관계에 있다면, 
공분산의 값은 양수가 될 것이다. 
반대로 2개의 변수중 하나의 값이 상승하는 경향을 보일 때, 다른 값이 하강하는 경향을 보인다면 
공분산의 값은 음수가 된다. 이렇게 공분산은 상관관계의 상승 혹은 하강하는 경향을 이해할 수 있으나 
2개 변수의 측정 단위의 크기에 따라 값이 달라지므로 상관분석을 통해 정도를 파악하기에는 부적절하다. 
이것을 보완하기 위해 상관계수라는 것을 사용하는데, 확률변수의 절대적 크기에 영향을 받지 않도록 하는 것.
```

![](https://raw.github.com/yoonkt200/DataScience/master/week5_PythonMachineLearning/week5_images/1.png)

>> 공분산 행렬의 이해

```
분산-공분산 행렬은 여러 변수와 관련된 분산과 공분산을 포함하는 정방형 행렬이다. 
(정방형 행렬은 행과 열의 개수가 동일한 행렬을 의미한다.)
행렬의 대각선 원소는 각 변수의 분산을 포함하며, 대각선 이외의 원소는 가능한 모든 변수 쌍 간의 공분산을 포함한다.
```

![](https://raw.github.com/yoonkt200/DataScience/master/week5_PythonMachineLearning/week5_images/2.png)

>> 고유값(eigenvalue)과 고유벡터(eigenvector)

```
행렬 A를 선형변환으로 봤을 때, 선형변환 A에 의한 변환 결과가 자기 자신의 상수배가 되는 0이 아닌 벡터를 
고유벡터(eigenvector)라 하고 이 상수배 값을 고유값(eigenvalue)라 한다.

즉, nxn 정방행렬(고유값, 고유벡터는 정방행렬에 대해서만 정의된다) A에 대해 Av = λv를 만족하는 
0이 아닌 열벡터 v를 고유벡터, 상수 λ를 고유값이라 정의한다.

좀더 정확한 용어로는 λ는 '행렬 A의 고유값', v는 '행렬 A의 λ에 대한 고유벡터'이다.

- 출처 : http://darkpgmr.tistory.com/105
```

![](https://raw.github.com/yoonkt200/DataScience/master/week5_PythonMachineLearning/week5_images/3.png)

```
PCA에 한정하여 고유값과 고유벡터를 살펴보면, 공분산 행렬 A를 알면 두 결과값을 얻을 수 있다는 것을 알수있다.

기하학적 의미를 아는 것 역시 중요하다.
고유값과 고유벡터의 기하학적 의미는, 고유벡터는 선형변환 A에 의해서 방향이 보존되며 크기만 변하는 벡터라는 것이고
고유값은 그 고유벡터의 변하는 크기를 나타내는 상수라고 할 수 있다.

이렇게 고유값 및 고유벡터에 대한 수학적인 내용이나 기하학적 의미를 대략적으로 파악하는 것도 꽤나 중요한 일이지만,
더 깊은 내용을 제대로 알기 위해서는 수학적인 내공이 필요한 것 같다.
```


-----------------------


> **1.2 선형 판별 분석을 활용한 지도적 데이터 압축**

